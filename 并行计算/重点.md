# Lecture 1
## What is High Performance Computing (HPC)
高性能计算 (High Performance Computing, HPC) 是指使用并行处理来运行程序的计算方法。

HPC的主要目标是：

- **效率** (Efficiency)
- **可靠性** (Reliability)
- **速度** (Speed)

## Why is HPC important?
高性能计算是一个非常活跃的研究领域。需要HPC的问题类型：

- **计算密集型** (Compute intensive): 需要大量计算的单个问题
- **内存密集型** (Memory intensive): 需要大量内存的单个问题
- **数据密集型** (Data intensive): 处理大量数据的单个问题
- **高吞吐量** (High Throughput): 长期执行的多个不相关问题


## How do we measure Computing?
计算机能够执行多种不同的计算，我们需要量化其性能：

- 最初使用**每秒指令数** (Instructions Per Second, IPS) 进行测量
- 现代测量通常使用**每秒浮点运算次数** (Floating Point Operations Per Second, FLOPS)
## Difference between Parallel and Concurrent Computing?
### 并行计算 (Parallel Computing)

并行计算是指<font color="#ff0000">同时执行多个计算或进程</font>：
Parallel computing is where many calculations or the execution of processes are carried out simultaneously

- 计算任务通常<font color="#548dd4">被分解成几个（通常是很多个）相似的子任务</font>
- 这些子任务可以独立处理
- 完成后，结果被合并

### 并发计算 (Concurrent Computing)

并发计算是指<font color="#ff0000">同时执行多个任务</font>：

- 这些任务通常<font color="#548dd4">不相关</font>
- 即使单处理器计算机也可以通过并发计算同时运行多个应用程序

### 并发与并行的区别 (Differences between Concurrent and Parallel Computing)

并行和并发计算的概念常被混淆：

- 可以有不带并行的并发
    - 例如：单CPU上的时间共享
- 可以有不带并发的并行
    - 例如：位级或指令级并行
- <font color="#548dd4">大多数现代计算机既支持并行又支持并发</font>

# Lecture 2

## Structure and Function of components
### 层次视图 (Hierarchical View)

- 计算机是复杂系统，最简单的描述方式是通过层次视图
- 最高层次上，计算机是一组相互关联的组件
- 每个组件深入研究，又由相互关联的子组件组成

### 结构与功能的关系

- **结构(Structure)**：组件间如何相互关联(系统设计的内容和方式)
- **功能(Function)**：组件如何在结构内运作(系统做什么和如何做)
- 结构使功能成为可能，例如：多核CPU(结构)使并行处理(功能)成为可能
- 功能影响结构，例如：高速数据处理需求促成了缓存内存的发展

## The stored-program concept and von Neumann machines
### 存储程序概念的核心

- 程序与数据一起存储在内存中
- 程序需要以二进制形式表示
- 计算机可以通过从内存读取来获取指令
### 冯·诺依曼架构 (von Neumann Machines)

- IAS计算机的结构是几乎所有现代计算机的基础
- 现代计算机虽然更复杂且有许多细微差别，但基本结构相似，仍被称为冯·诺依曼机
- 一种将程序指令存储器和数据存储器合并在一起的存储器结构
## Designing for Performance (eﬀiciency techniques)
### 效率技术 (Efficiency Techniques)

随着时钟频率提高变得越来越困难，采用了不同技术来提高效率：

1. **流水线技术** (Pipelining)：允许处理器同时处理多条指令，每条指令处于指令周期的不同部分
2. **分支预测** (Branch Prediction)：处理器预测哪些指令分支可能被处理，提前获取正确指令
3. **数据流分析** (Data Flow Analysis)：分析指令间的数据依赖关系，创建优化的指令调度
4. **推测执行** (Speculative Execution)：提前执行可能需要的指令，将结果保存在临时位置

## Interconnection structures and memory
### 总线互连 (Bus Interconnections)

- **总线**：连接两个或多个设备的通信路径
- 特点：共享传输媒介，多个设备连接到同一总线
- 当一个设备传输时，所有其他设备都能收到；如果两个设备同时传输，信息会受到干扰

### 总线的组成与性能

- 总线是连接到所有设备的一组并行导线
- **总线宽度**(Bus Width)：总线一次能传输的位数，常限制计算机性能
- **总线速度**(Bus Speed)：以每秒周期数(Hz)计量

### 总线的问题

- 连接的设备越多，同时传输的可能性越大
- 导致更长的导线，传输速度更慢
- 协调控制总线使用困难

### 点对点互连 (Point-to-Point Interconnects)

- 作为总线的替代方案
- 不是单一共享传输媒介，而是在组件对之间有许多单独连接
- 不需要协调传输

缓存内存 (Cache Memory)

### 内存系统特征

- 内存位置：寄存器、缓存、主内存、SSD、磁盘、光盘等
- 存储容量：以字节或字为单位
- 传输量：总线或块的大小
- 访问方式：顺序、直接、随机、关联

### 内存层次结构 (Memory Hierarchy)

- 计算机内存设计受三个约束：容量多大、速度多快、成本多高
- 存在容量、访问时间和成本之间的权衡：
    - 更快的访问时间，每位成本更高
    - 更大的容量，每位成本更低
    - 更大的容量，访问时间更慢
- 解决方案是使用内存层次结构

### 缓存内存工作原理

- 缓存是一小块非常快速的内存
- 存储常用项目以提高整体性能
- CPU需要数据时，先检查缓存：
    - 如果数据在缓存中(命中)，则更快访问
    - 如果不在(未命中)，则从内存读取数据到缓存

### 多级缓存 (Multi Level Caches)

- 缓存通常组织成多个级别
- 较小且靠近CPU的缓存更快，较大且距离更远的缓存较慢

# Parallel Computing

## Parallel and Sequential Computing

### 串行计算 (Serial Computing)

- 传统的软件编写方式，也称为**顺序计算**(sequential computation)
- 任务被分解为一系列指令
- 这些指令**一个接一个地执行**(executed one after another)
- 任何时刻只能执行一条指令
- 假设只有一个CPU
- 先执行t1，然后t2，然后t3，以此类推

### 并行执行 (Parallel Execution)

- 在并行计算中，我们**同时使用多个计算资源**(use multiple compute resources at the same time)解决单个问题
- 需要**多个处理器、核心或GPU**(multiple processors, cores, or GPUs)
- 任务被分解为可以并行解决的部分
- 每个部分进一步分解为一系列指令
- 来自每个部分的指令同时执行

### 并行计算资源 (Parallel Computing Resources)

**硬件计算资源**包括：

- 具有多个处理器的单台计算机（或具有多个核心的单个处理器）
- 具有专门计算资源的计算机，如**GPU、FPGA或协处理器**(GPUs, FPGAs, or Coprocessors)
- 通过网络连接的多台计算机
- 以上任意组合

**软件计算资源**包括：

- 并行编程模型：线程化、多处理(Threading, Multiprocessing)(如C中的pThreads、Java线程、Python多处理)
- 并行计算框架：**CUDA、OpenMP、MPI**(CUDA, OpenMP, MPI)

### 并行计算问题 (Parallel Computational Problems)

- 不是每个问题都可以并行解决
    - 例如：许多哈希函数等常见密集算法无法并行化
- 许多可以并行解决的问题可能不会看到好处
    - 过程中的其他部分可能存在瓶颈

**可并行算法通常具有以下特点**：

- 可以分解为离散的工作片段
- 这些片段可以同时执行
- 使用多个计算资源解决时耗时更少

**不适合并行化的问题类型**：

1. **固有顺序问题**(Inherently Sequential Problems)：数据依赖要求顺序执行
2. **高同步开销**(High Synchronization Overhead)：锁争用降低效率
3. **内存受限问题**(Memory-Bound Problems)：受内存访问速度限制，而非计算限制
4. **低计算复杂度**(Low Computational Complexity)：任务完成速度太快，并行无益
5. **高通信开销**(High Communication Overhead)：线程/进程间通信过多

### 为什么需要并行计算 (Why Parallel Computing)

创建可并行执行的算法很困难，我们这样做的原因包括：

- 节省时间（壁钟时间wall clock time）
- 在相同时间内解决更大的问题
- 同时解决多个任务
- 能够使用非本地资源
- 通过使用许多更便宜、功率较低的设备获得更多处理能力
- 解决对单个设备内存来说过大的问题

### 未来趋势 (The Future)

- 最近几十年的重点从更快的时钟速度转向CPU中的更多核心
- AI和其他密集型应用的日益使用导致GPU大规模发展
- 许多重要技术（如机器学习、云计算、大数据）都依赖于并行计算

## Limitations of Serial Computing
### 串行计算的主要限制

- 传输速度限制(Transmission speeds)
- 微型化限制(Miniaturisation)
- 经济限制(Economy)
- 空间和热量限制(Space and heat)

### 传输速度限制 (Transmission Speed Limitations)

- 串行计算机的速度直接取决于数据通过硬件的速度
- 绝对限制是光速（30厘米/纳秒）
- 铜线的传输限制是9厘米/纳秒
- 要增加数据传输，我们必须：
    - 使组件靠得更近
    - 或发明新的更快的信息媒介

### 微型化限制 (Miniaturisation Limitations)

- 处理器中的晶体管越来越小
    - 许多当前一代处理器的晶体管尺寸以纳米(<15nm)计
- 这允许在每个芯片上放置越来越多的晶体管
- 最终，组件的小型化将达到极限
    - 我们不能小于单个原子

### 经济限制 (Economic Limitations)

- 公司制造处理器的程序非常复杂且昂贵
- 人们愿意为处理器支付的价格有限
    - 特别是如果你可以用一个更快的处理器的价格购买并使用10个更慢的处理器

### 空间和热量限制 (Space and Heat Limitations)

- 我们已经看到，通过将组件靠近可以提高传输速度
- 但是对于我们可以放置组件的小型化/接近程度有限制
- 另一个限制是组件密度越高，产生的热量就越多！
- 高性能CPU通常需要大型散热器和风扇，只是为了防止过热

## Flynn’s Taxonomy
### 弗林分类法概述

- 并行计算机最广泛使用的分类之一
- 弗林分类法沿着两个独立维度区分多处理器计算机架构
    - 指令(Instruction)
    - 数据(Data)
- 这些维度中的每一个只能有两种可能状态之一：单个(Single)或多个(Multiple)

### 分类矩阵 (Matrix)

弗林分类法定义了4种可能的分类：

|   |   |   |
|---|---|---|
||单数据(Single Data)|多数据(Multiple Data)|
|单指令(Single Instruction)|SISD|SIMD|
|多指令(Multiple Instruction)|MISD|MIMD|

### 单指令流单数据流 (SISD)

- 串行（非并行）计算机
- **单指令**：在任何一个时钟周期内，CPU仅处理一个指令流
- **单数据**：在任何一个时钟周期内，仅使用一个数据流作为输入
- 这种系统中的执行始终是确定性的
    - 相同的代码将始终产生相同的结果
- 这是最古老的，直到最近，最普遍的计算机架构形式
- 大多数计算机和笔记本电脑直到10-15年前

### 单指令流多数据流 (SIMD)

- 一种并行计算机
- **单指令**：所有处理单元在任何给定时钟周期执行相同的指令
- **多数据**：每个处理单元可以对不同的数据元素进行操作
- 同步（锁步）和确定性执行
- 这种机器通常有一个指令调度器、一个非常高带宽的内部网络和一个非常大的、容量非常小的指令单元阵列
- 最适合具有高度规律性的专业问题，如图像处理
- 两种变体：处理器阵列(Processor Arrays)和向量管道(Vector Pipelines)

### 多指令流单数据流 (MISD)

- 单个数据流被送入多个处理单元
- 每个处理单元通过独立的指令流独立地对数据进行操作
- 这类并行计算机的实际例子很少

### 多指令流多数据流 (MIMD)

- 当前最常见的并行计算机类型
- 大多数现代计算机属于这一类
- **多指令**：每个处理器可能正在执行不同的指令流
- **多数据**：每个处理器可能正在处理不同的数据流
- 执行可以是同步的或异步的，确定性的或非确定性的
- 大多数当前的超级计算机、网络并行计算机"网格"和多处理器SMP计算机以及许多PC都是MIMD

# Lecture 4

## Parallel Memory Architecture (Shared Vs. Distributed)
## Shared Memory Architecture Overview

Shared memory is a parallel computer architecture where all processors have access to a global memory address space. This allows multiple processors to operate independently while sharing common memory resources. When one processor modifies a value in memory, that change is visible to all other processors, providing implicit communication between processes.

## Uniform Memory Access (UMA)

UMA is a shared memory architecture where all processors have **equal access time to all memory locations**.

### Key Characteristics of UMA:

1. **Symmetric Multiprocessor (SMP) Implementation**: Most commonly represented by SMP machines where all processors are identical.
2. **Equal Memory Access**: All processors have uniform access times to all memory locations.
3. **Cache Coherence**: Often referred to as CC-UMA (Cache Coherent UMA), this architecture ensures that all caches maintain a consistent view of memory.
4. **Memory Access Pattern**: Memory access time is constant regardless of which processor makes the request or which memory location is accessed.
5. **Structure**: Typically consists of multiple processors connected to a central memory through an interconnect.

### Cache Coherence in UMA

Cache coherence mechanisms ensure that:

- When one processor updates a memory location in its cache, all other processors see this update
- Multiple processors don't have conflicting versions of the same memory location
- Coherence protocols (like MESI - Modified, Exclusive, Shared, Invalid) track the state of cache lines

### Advantages of UMA:

- Simpler programming model due to uniform access times
- Lower latency for memory operations
- Easier to implement and manage
- Good performance for applications with uniform memory access patterns

### Limitations of UMA:

- Limited scalability (typically effective up to 8-16 processors)
- Performance degradation as the number of processors increases due to memory bus contention
- Potential bottlenecks at the interconnect as system size grows

## Non-Uniform Memory Access (NUMA)

NUMA is a shared memory architecture where memory access time depends on the memory location relative to the processor.

### Key Characteristics of NUMA:

1. **Distributed Shared Memory**: Memory is physically distributed among processors but logically shared.
2. **Variable Access Times**: Access time to memory varies based on the "distance" between the processor and memory module.
3. **Node Structure**: NUMA systems are organized into nodes, where each node contains processors and local memory.
4. **Inter-node Communication**: Nodes are connected via a high-speed interconnect that allows processors to access memory on other nodes.
5. **Local vs. Remote Access**: Accessing local memory (within the same node) is faster than accessing remote memory (in different nodes).

### Implementation of NUMA:

- Often created by linking multiple SMP systems together
- One SMP can directly access the memory of another SMP, but with higher latency
- Modern server CPUs with multiple sockets typically implement NUMA architecture

### Advantages of NUMA:

- Greater scalability than UMA (can scale to hundreds of processors)
- Better memory utilization and bandwidth
- Reduced contention on the memory bus
- Higher overall system performance for properly optimized applications

### Limitations of NUMA:

- More complex programming model
- Performance heavily depends on data locality
- Memory access patterns can significantly impact performance
- Requires NUMA-aware operating systems and applications for optimal performance

## Comparison Between UMA and NUMA

|Feature|UMA|NUMA|
|---|---|---|
|Memory Access Time|Uniform for all processors|Varies based on memory location|
|Scalability|Limited (typically ≤16 processors)|Higher (can scale to hundreds)|
|Programming Complexity|Lower|Higher|
|Hardware Implementation|Simpler|More complex|
|Performance Predictability|More predictable|Varies based on memory access patterns|
|Cache Coherence|Easier to implement|More complex across nodes|
|Typical Use Cases|Smaller servers, workstations|Large enterprise servers, HPC systems|

## Programming Considerations

1. **UMA Programming**:
    
    - Simpler programming model
    - Focus on balanced workload distribution
    - Synchronization is the primary concern
2. **NUMA Programming**:
    
    - Focus on data locality (keeping data close to the processor that uses it)
    - Memory affinity (binding processes to specific NUMA nodes)
    - NUMA-aware memory allocation
    - Avoiding frequent cross-node memory access

## Practical Applications

1. **UMA Systems**:
    
    - Desktop computers with multi-core processors
    - Small to medium servers
    - Applications with uniform memory access patterns
2. **NUMA Systems**:
    
    - Enterprise servers with multiple CPU sockets
    - High-performance computing clusters
    - Large database servers
    - Applications that can be partitioned with good data locality

## Distributed Memory (characteristics, advantages, disadvantages)

### 分布式共享内存基本概念 (Basic Concept of Distributed Shared Memory)

分布式共享内存是结合了分布式和共享内存特点的混合架构：

- 每个节点有自己的**私有内存** (private memory)
- 所有节点都可以访问一个**共享内存空间** (shared memory space)
- 在物理上是分布式的，但在逻辑上呈现为共享的
- 也称为**虚拟共享内存** (Virtual Shared Memory)

### 分布式共享内存的实现 (Implementation of Distributed Shared Memory)

分布式共享内存可以通过硬件或软件实现：

- **硬件实现**：特殊的内存控制器和网络硬件
- **软件实现**：操作系统或库提供共享内存抽象

### 分布式共享内存的应用 (Applications of Distributed Shared Memory)

分布式共享内存在实际中应用相对较少：

- 这是一种比较**特殊的架构** (exotic architecture)
- 不在主流高性能计算系统中广泛使用
- 在一些特定应用中有价值，如需要大规模共享数据的应用

### 分布式共享内存的优缺点 (Advantages and Disadvantages of DSM)

分布式共享内存结合了两种架构的特点：

- **优势**：编程模型简单（类似共享内存），同时提供一定的可扩展性
- **劣势**：实现复杂，性能可能不如纯粹的共享内存或分布式内存系统

# Lecture 4 Parallel Programming Models
共享内存 (Shared Memory)

**共享内存特点** (Characteristics of Shared Memory)：

- 所有处理器可以访问**全局地址空间** (global address space)
- 多个处理器可以独立操作但共享相同的内存资源
- 当一个处理器更改内存位置中的值时，该更改对所有其他处理器可见（**隐式通信** implicit communication）

**共享内存分类**：

- **均匀内存访问** (Uniform Memory Access, UMA)
    
    - 所有处理器对内存的访问时间相同
    - 通常由**对称多处理器系统** (Symmetric Multiprocessor, SMP)代表
    - 所有处理器相同，具有相等的内存访问权限和访问时间
    - 有时称为**缓存一致性UMA** (Cache Coherent UMA, CC-UMA)
    
    想象一栋公寓楼：
    
    - 这栋楼里住着很多人（处理器）
    - 楼里有一个公共娱乐室（共享内存）
    - 每个住户从自己房间到娱乐室的距离**完全相同**
    - 所有人走到娱乐室的时间都一样快
    - 每个人都能平等使用娱乐室里的所有设施
    
    这就是UMA系统：所有处理器访问内存的速度一样快，没有谁比谁有优势。
    
- **非均匀内存访问** (Non-Uniform Memory Access, NUMA)
    
    - 通常通过物理链接两个或多个SMP系统构成
    - 一个SMP可以直接访问另一个SMP的内存
    - 并非所有处理器对所有内存都有相同的访问时间
    - 通过另一个SMP访问的内存位置需要更长时间
    
      
    
    度假村有多栋公寓楼（多个SMP系统）
    
    - 每栋楼都有自己的娱乐室（本地内存）
    - 住在1号楼的人去1号楼的娱乐室很快
    - 但如果1号楼的人想去2号楼的娱乐室，就需要走一段路，花更多时间
    - 虽然所有人都能使用所有楼的娱乐室，但去自己楼的娱乐室明显更方便
    
    这就是NUMA系统：处理器访问自己"附近"的内存很快，但访问"远处"的内存就会变慢。
    

## 线程模型 (Threads Model)

**线程模型概述** (Overview)：

- 线程模型允许单个进程有**多个并发执行路径** (multiple concurrent execution paths)
    - 这些执行路径称为线程
- 进程通常被描述为"**重量级**"(heavy weight)
- 线程通常被描述为"**轻量级**"(light weight)

**进程定义** (Process Definition)：

- 进程是计算机正在执行的程序实例
    - 典型计算机上通常运行许多进程
- 进程负责管理程序的执行
- 包含：
    - 程序指令
    - 程序数据（堆栈、静态和堆）
    - 一些寄存器和程序计数器

**线程模型优势** (Advantages)：

- 每个线程有**本地数据**（自己的堆栈），但共享进程的资源
    - 程序代码只需存储一次！（更少开销）
    - 线程间共享内存的好处
- 线程通过共享内存通信
    - 这要求程序员考虑同步
- 线程可以频繁启动和完成，但主进程必须继续执行

1. **资源共享效率高**：
    - 每个线程有自己的本地数据（自己的堆栈），但共享进程的资源
    - 程序代码只需存储一次，减少了内存开销
    - 线程之间通过共享内存进行通信，速度快
2. **灵活性**：
    - 线程可以频繁启动和完成，但主进程可以继续执行
    - 可以根据需要动态调整线程数量
3. **简化编程**：
    - 相比进程间通信，线程间的数据共享更加简单直接

**线程模型实现** (Implementations)：

- 线程模型有许多实现
- 通常以两种方式实现：
    - 从并行源代码内部调用的**子例程库** (library of subroutines)
    - 嵌入在源代码中的**指令集** (set of directives)（例如OpenMP）
- 两个非常知名的实现：**POSIX线程**和**OpenMP**

**POSIX线程** (POSIX Threads)：

- 由IEEE POSIX 1003.1c标准（1995）规定。仅C语言。
- Unix/Linux操作系统的一部分
- 基于库
- 通常称为Pthreads
- 非常**显式的并行性**；需要程序员高度关注细节

**OpenMP**：

- 基于**编译器指令**；可以使用串行代码
- 由一组主要计算机硬件和软件供应商共同定义和认可
- 可移植/多平台，包括Unix和其他平台
- 有C/C++和Fortran实现
- 可以非常**容易和简单**使用 - 提供"增量并行"


## Distributed Memory Model: Sending and Receiving, characteristics
**缓存一致性** (Cache Coherence)：

- 共享内存系统可能出现的问题是内存块的缓存可能过期
- 如果另一个处理器更改了我们缓存中的块，就可能发生这种情况
- **缓存一致性共享内存系统**有硬件确保这不可能发生
- 每当缓存中的块被更改时，缓存也会更新

**共享内存优势** (Advantages)：

- **全局地址空间**提供用户友好的内存编程视角
    - 无论连接到哪个处理器，内存地址没有区别
- 任务间的**数据共享**因内存靠近CPU而快速且统一

**共享内存劣势** (Disadvantages)：

- 内存与CPU之间的**可扩展性**不足
    - 添加更多CPU会几何级增加共享内存-CPU路径上的流量
- 程序员负责**同步结构**以确保"正确"访问全局内存
- 随着处理器数量增加，设计和生产共享内存机器变得越来越困难和昂贵


分布式内存/消息传递模型 (Distributed Memory/Message Passing Model)

**分布式内存/消息传递模型概述** (Overview)：

- 这种模型设计用于**无共享内存环境**的并行编程
- 任务/进程各自有自己的本地内存
    - 这可能分布在不同机器上
- 通过**显式发送和接收的消息**交换数据

**发送和接收** (Sending and Receiving)：

- 为了能够向另一个进程发送数据，它必须准备好接收
- 进程需要调用某种接收子程序
    - 然后它必须等待发送进程传输数据
- 发送和接收数据必须协调

**消息传递模型特点** (Characteristics)：

- 从编程角度看，消息传递实现通常包括子程序库
- 对这些子程序的调用嵌入在源代码中
- 程序员负责确定所有并行性

**消息传递模型实现** (Implementations)：

- 消息传递模型有很多实现
- 开发了一个标准接口，称为**消息传递接口** (Message Passing Interface, MPI)
    - 1994年首次发布
- MPI已成为消息传递的行业标准
- 当前版本是MPI-4.1，MPI-5.0正在开发中

**MPI特点** (MPI Features)：

- MPI专为C、C++和FORTRAN设计
- 但它也有其他语言的绑定：Java、MATLAB、OCaml、Python、R等
- MPI设计用于在许多不同架构上工作
- 我们程序中相同的代码会被调整为以最高效的方式运行
    - 如果代码在共享内存系统上运行，则会复制相关内存
    - 如果相同代码在分布式内存系统上运行，则相关数据将通过网络传输
    - 无论如何，我们编写相同的代码！


# Designing Parallel Programs
## 一、理解问题 (Understand the Problem)

### 1. 可并行化和不可并行化问题 (Parallelisable and Non-Parallelisable Problems)

#### 可并行化问题的特征:

- **可分解性**: 可以被分解为离散的工作单元
- **独立执行**: 这些单元可以同时执行
- **性能提升**: 使用多个计算资源能缩短解决时间

#### 可并行化问题示例:

- **图像处理**: 如人脸检测，每个图像区块可以独立并行处理
- **矩阵运算**: 矩阵乘法中的元素计算可以并行
- **模拟计算**: 粒子模拟、流体动力学模拟
- **3D渲染**: 每个像素或帧可以独立计算
- **数据分析**: 大数据集的独立分析处理

#### 不可并行化问题特征:

- **顺序依赖性**: 当前步骤依赖于前一步骤的结果
- **无法分解**: 问题本质上是串行的，不能分解为可并行的组件

#### 不可并行化问题示例:

- **斐波那契数列计算**: 每个数依赖于前两个数
- **链式依赖算法**: 如某些哈希函数
- **某些图遍历算法**: 需要按特定顺序访问节点

#### 不适合并行化的情况:

- **同步开销高**: 线程锁争用降低效率
- **内存受限问题**: 受内存访问速度限制而非计算能力
- **计算复杂度低**: 任务完成太快，并行化反而增加开销
- **通信开销高**: 线程/进程间通信过多

### 2. 关键概念定义与示例

#### Hotspots (热点):

- **定义**: 程序中计算密集型区域，消耗大量CPU时间
- **特点**:
    - 是并行化的首要目标
    - 通常是循环或重复执行的代码块
- **示例**:
    - 图像处理中的滤波器应用
    - 科学计算中的迭代求解
    - 深度学习中的矩阵乘法运算

#### Bottlenecks (瓶颈):

- **定义**: 不成比例地减慢程序执行或导致并行工作停滞的区域
- **特点**:
    - 限制整体系统性能
    - 可能需要算法重构或特殊处理
- **示例**:
    - I/O操作（文件读写）
    - 网络通信延迟
    - 串行代码段
    - 资源争用（如内存带宽）
- **解决方案**:
    - 重构程序
    - 使用不同算法
    - 采用并行文件系统（如Lustre）提高I/O性能

#### Inhibitors (抑制因素):

- **定义**: 阻碍有效并行化的因素
- **主要类型**:
    - 数据依赖性: 一个操作需要另一个操作的结果
    - 控制依赖性: 程序流程取决于前面的决策
    - 资源依赖性: 多个处理单元竞争同一资源
- **示例**:
    - 循环携带依赖(Loop-carried dependencies)
    - 顺序算法（如动态规划中的某些问题）
    - 共享资源竞争

#### Serial Components (串行组件):

- **定义**: 无法并行化的程序部分
- **特点**:
    - 大多数并行代码中仍有大量串行部分
    - 这些部分限制了整体可达到的加速比(根据Amdahl定律)
- **处理方法**:
    - 虽然不能并行化，但可以进行优化

## 二、问题分解 (Partitioning)

### 1. Domain Decomposition (领域分解)

- **定义**: 将数据分割成多个部分，每个部分由单独的任务处理
- **适用场景**:
    - 数据密集型应用
    - 规则结构的数据集
- **实现方式**:
    - 几何分解: 基于空间位置分割数据(如网格、矩阵)
    - 递归分解: 递归地将问题分解为更小的子问题
- **示例**:
    - 矩阵计算: 将矩阵分块处理
    - 图像处理: 将图像分割为多个区域
    - 粒子模拟: 空间区域划分

### 2. Functional Decomposition (功能分解)

- **定义**: 基于功能或任务类型划分工作，关注计算类型而非数据
- **适用场景**:
    - 复杂的多阶段处理流程
    - 异构计算环境
- **实现方式**:
    - 流水线方式: 不同阶段由不同处理单元执行
    - 主从模式: 主进程分配任务给从进程
- **示例**:
    - 图形渲染流水线
    - 编译器的不同阶段(词法分析、语法分析等)
    - 复杂数据处理流程(如ETL过程)

### 3. 两种分解方法的比较

- **Domain Decomposition**:
    - 优势: 数据局部性好，通信模式规则，负载均衡容易实现
    - 劣势: 对不规则数据结构效果较差
- **Functional Decomposition**:
    - 优势: 适合复杂处理流程，可以利用专用硬件特性
    - 劣势: 可能导致负载不均衡，通信模式复杂

## 三、通信 (Communications)

### 1. 必要与非必要通信

- **必要通信**:
    
    - **定义**: 为完成计算必不可少的数据交换
    - **特点**: 无法避免，但可以优化
    - **示例**:
        - 边界数据交换(如偏微分方程求解)
        - 全局归约操作(如求和、查找最大值)
        - 主进程向工作进程分发任务
- **非必要通信**:
    
    - **定义**: 可以通过算法或数据结构改进来减少或消除的通信
    - **原因**:
        - 算法设计不当
        - 数据分布不合理
        - 冗余计算不足
    - **示例**:
        - 重复传输相同数据
        - 可本地计算的值进行远程获取
        - 过于频繁的同步

### 2. 通信范围 (Scope of Communications)

- **点对点通信 (Point-to-Point)**:
    
    - 两个进程之间的直接通信
    - MPI实现: MPI_Send, MPI_Recv
    - 适用: 不规则通信模式，动态负载均衡
- **集体通信 (Collective)**:
    
    - 涉及多个进程的协调通信
    - MPI实现: MPI_Broadcast, MPI_Reduce, MPI_Gather
    - 适用: 全局操作，规则通信模式

### 3. 通信优化策略

- **减少通信频率**: 合并多次小通信为一次大通信
- **重叠通信与计算**: 在等待通信完成时执行其他计算
- **利用局部性**: 优先处理不需要远程数据的计算
- **异步通信**: 使用非阻塞通信减少等待时间
- **选择合适的通信模式**: 点对点vs集体通信

## 四、同步 (Synchronisation)

### 1. 同步类型

- **屏障同步 (Barrier)**:
    
    - 所有进程/线程在某点等待，直到全部到达
    - 实现: MPI_Barrier, OpenMP #pragma omp barrier
    - 用途: 确保所有进程完成一个阶段才开始下一阶段
- **互斥锁 (Mutex)**:
    
    - 保护共享资源不被多个线程同时访问
    - 实现: OpenMP临界区，pthread_mutex
    - 用途: 避免数据竞争和不一致
- **信号量 (Semaphore)**:
    
    - 控制对有限资源的访问
    - 用途: 生产者-消费者问题，资源池管理
- **条件变量 (Condition Variable)**:
    
    - 允许线程等待特定条件发生
    - 用途: 复杂的线程协调

### 2. 处理数据依赖性

- **分布式内存架构**: 在同步点通过通信传递所需数据
- **共享内存架构**: 同步读/写操作，使用互斥锁或原子操作
- **循环携带依赖性的处理**:
    - 重构算法减少依赖
    - 使用特殊的并行模式(如扫描操作)

## 五、粒度 (Granularity)

### 1. 细粒度并行性 (Fine-Grained Parallelism)

- **定义**: 问题被分解为大量小任务
- **特点**:
    - 可能导致大量进程，增加通信和同步开销
    - 适合支持快速通信的架构
    - 负载均衡效果好
- **示例**:
    - GPU计算中的每个像素处理
    - 每个矩阵元素的操作

### 2. 粗粒度并行性 (Coarse-Grained Parallelism)

- **定义**: 问题被分解为少量大任务
- **特点**:
    - 通信和同步开销较小
    - 可能导致负载不均衡
    - 适合通信开销大的系统
- **示例**:
    - 分布式数据库中的分片处理
    - 大块数据的独立处理

### 3. 选择合适粒度的考虑因素

- **计算与通信比率**: 粒度应使计算量远大于通信开销
- **系统架构**: 通信延迟高的系统适合粗粒度
- **负载均衡需求**: 细粒度有助于动态负载均衡
- **问题特性**: 自然分解的粒度大小

## 六、并行编程的限制与成本

### 1. Amdahl定律

- **公式**: Speedup = 1 / [(1-P) + P/N]
    - P: 可并行化的程序比例
    - N: 处理器数量
- **含义**: 串行部分限制了可达到的最大加速比
- **示例**: 如果90%可并行化，8个处理器的最大加速比约为4.7

### 2. 并行开销来源

- **通信开销**: 数据传输时间和带宽限制
- **同步开销**: 进程/线程等待和协调
- **负载不均衡**: 进程/线程工作量不平衡
- **额外计算**: 并行算法可能需要额外操作
- **内存开销**: 复制数据，创建额外数据结构

### 3. I/O考虑

- **I/O通常是瓶颈**
- **并行文件系统可提高性能**
- **减少I/O操作的策略**:
    - 批处理I/O请求
    - 使用缓冲减少I/O频率
    - 将I/O操作与计算重叠

## 总结

设计有效的并行程序需要系统性思考以下几个关键方面:

1. **问题特性分析**: 识别热点、瓶颈和抑制因素
2. **合理的分解策略**: 选择适当的领域分解或功能分解方法
3. **高效的通信模式**: 最小化不必要通信，优化必要通信
4. **适当的同步机制**: 确保正确性的同时降低同步开销
5. **合适的并行粒度**: 平衡计算与通信的比率
6. **I/O与其他瓶颈处理**: 识别和优化关键瓶颈
7. **充分理解并行架构**: 根据目标架构特性调整设计

# 并行程序设计与并行编程详细笔记

## 一、通信范围 (Scope of Communications)

并行程序设计中，了解任务间的通信模式是关键。通信范围主要分为两种类型：

### 1. 点对点通信 (Point-to-Point Communication)

- **定义**：涉及两个任务之间的直接通信，一个任务作为数据的发送者/生产者，另一个作为接收者/消费者
- **特点**：
    - 直接在特定进程间传递数据
    - 适用于不规则的通信模式
    - 基于发送和接收操作配对
- **实现方式**：
    - 在MPI中通过`MPI_Send()`和`MPI_Recv()`函数实现
    - 发送方需要知道接收方的进程ID（rank）
    - 接收方需要知道从哪个进程接收数据

### 2. 集体通信 (Collective Communication)

- **定义**：涉及两个以上任务之间的数据共享，这些任务通常被指定为共同组或集体的成员
- **特点**：
    - 所有进程必须参与集体操作
    - 提供比点对点通信更高效的实现
    - 适合规则的数据分布和收集模式
- **实现方式**：
    - 在MPI中提供了一系列集体通信函数
    - 典型操作包括广播、收集、分散和规约等

## 二、同步类型 (Types of Synchronisation)

同步机制用于协调并行任务的执行，确保正确性和一致性：

### 1. 屏障同步 (Barrier Synchronisation)

- **定义**：强制所有线程/进程在某一点等待，直到所有进程都到达该点
- **特点**：
    - 用于确保所有进程完成一个阶段后再继续
    - 可能导致进程空闲等待
- **实现方式**：
    - MPI中使用`MPI_Barrier()`
    - OpenMP中使用`#pragma omp barrier`

### 2. 互斥锁 (Mutex/Lock)

- **定义**：控制对共享资源的访问，确保同一时间只有一个线程可以访问该资源
- **特点**：
    - 线程拥有锁的所有权
    - 只有获得锁的线程才能解锁
    - 用于防止数据竞争
- **实现方式**：
    - POSIX线程库中的pthread_mutex
    - OpenMP中的`#pragma omp critical`

### 3. 信号量 (Semaphore)

- **定义**：控制对有限资源的访问，允许多个线程同时访问，但有上限
- **特点**：
    - 可以有多个初始值
    - 适用于生产者-消费者问题
    - 比互斥锁更灵活
- **实现方式**：
    - POSIX提供的sem_t类型
    - 通过"P"（减少）和"V"（增加）操作控制

### 4. 条件变量 (Condition Variable)

- **定义**：允许线程等待特定条件发生
- **特点**：
    - 通常与互斥锁结合使用
    - 用于线程间的信号通知
    - 避免忙等待（busy waiting）
- **实现方式**：
    - POSIX的pthread_cond_t
    - 条件变量需要一个相关的互斥锁

### 5. 同步通信 (Synchronous Communication)

- **定义**：通过消息传递实现的同步
- **特点**：
    - 发送者等待接收者接收消息
    - 提供隐式同步
- **实现方式**：
    - MPI中的`MPI_Ssend()`（同步发送）

## 三、粒度 (Granularity)

粒度指并行任务分解的规模大小，影响通信开销和负载平衡：

### 1. 细粒度并行 (Fine-Grained Parallelism)

- **定义**：问题被分解为大量小任务
- **特点**：
    - 任务小且数量多
    - 通信频繁
    - 负载均衡效果好
    - 可能导致较高的通信和同步开销
- **实例**：
    - 矩阵计算中处理单个元素
    - 图像处理中处理单个像素
    - 并行排序算法中的细粒度划分

### 2. 粗粒度并行 (Coarse-Grained Parallelism)

- **定义**：问题被分解为少量大任务
- **特点**：
    - 任务大且数量少
    - 通信较少
    - 计算与通信比率高
    - 可能导致负载不均衡
- **实例**：
    - 处理大块矩阵数据
    - 图像处理中处理整个图像子区域
    - 处理数据集的独立子集

### 3. 中粒度并行 (Medium-Grained Parallelism)

- **定义**：介于细粒度和粗粒度之间的折中方案
- **特点**：
    - 平衡通信开销和计算效率
    - 适合大多数实际应用
- **实例**：
    - 处理矩阵的行或列
    - 处理图像的块区域

### 4. 粒度选择考虑因素

- 硬件架构（共享内存vs分布式内存）
- 通信开销
- 任务依赖关系
- 负载均衡需求
- 问题规模

## 四、并行编程的限制与成本

### 1. Amdahl定律 (Amdahl's Law)

- **定义**：量化使用多处理器时程序可能获得的加速比
    
- **公式**：
    
    ```
    Speedup(N) = 1 / ((1-P) + P/N)  
    ```
    
    其中：
    
    - N是处理器数量
    - P是可并行化的程序比例
    - (1-P)是串行部分比例
- **效率计算**：
    
    ```
    Efficiency = Speedup(N) / N  
    ```
    
- **公式解释**：
    
    - 即使处理器数量无限增加，最大加速比受串行部分限制
    - 最大理论加速比为1/(1-P)
    - 当P=0.9（90%可并行）时，最大加速比为10倍
    - 当P=0.99（99%可并行）时，最大加速比为100倍

### 2. 如何解读这些结果

- **串行部分的重要性**：
    
    - 程序中哪怕很小的串行部分也会限制最大可能的加速比
    - 增加处理器数量时会有收益递减
- **效率下降**：
    
    - 随着处理器数量增加，效率通常会下降
    - 需要平衡处理器数量和实际性能提升
- **实际应用价值**：
    
    - 帮助确定值得优化的程序部分
    - 评估投入额外计算资源的回报
    - 决定是否需要重构算法以提高可并行化比例

### 3. 并行开销来源

- **通信开销**：进程间数据传输时间
- **同步开销**：等待所有进程到达同步点
- **负载不均衡**：不同进程工作量不等
- **内存访问延迟**：内存访问竞争
- **额外计算**：并行算法可能需要额外工作

## 五、MPI并行编程

### 1. 点对点通信 (Point-to-Point Communication)

#### MPI_Send() 与 MPI_Recv() 函数

- **MPI_Send() 函数**：
    
    ```c
    int MPI_Send(  
        void *buf,           // 发送缓冲区起始地址  
        int count,           // 要发送的元素数量  
        MPI_Datatype datatype, // 发送数据类型  
        int dest,            // 目标进程的rank  
        int tag,             // 消息标签  
        MPI_Comm comm        // 通信域  
    )  
    ```
    
- **MPI_Recv() 函数**：
    
    ```c
    int MPI_Recv(  
        void *buf,           // 接收缓冲区起始地址  
        int count,           // 最大可接收元素数量  
        MPI_Datatype datatype, // 接收数据类型  
        int source,          // 源进程的rank（或MPI_ANY_SOURCE）  
        int tag,             // 消息标签（或MPI_ANY_TAG）  
        MPI_Comm comm,       // 通信域  
        MPI_Status *status   // 状态对象  
    )  
    ```
    

#### 代码示例：点对点通信

```c
#include <stdio.h>  
#include <mpi.h>  

int main(int argc, char** argv) {  
    int rank, size;  
    int number;  
    MPI_Status status;  
    
    // 初始化MPI环境  
    MPI_Init(&argc, &argv);  
    
    // 获取进程数量和当前进程rank  
    MPI_Comm_size(MPI_COMM_WORLD, &size);  
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);  
    
    if (size < 2) {  
        printf("本程序需要至少2个进程运行\n");  
        MPI_Finalize();  
        return 1;  
    }  
    
    // 进程0发送数据  
    if (rank == 0) {  
        number = 123;  
        MPI_Send(&number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);  
        printf("进程0发送数据: %d\n", number);  
    }  
    // 进程1接收数据  
    else if (rank == 1) {  
        MPI_Recv(&number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);  
        printf("进程1接收到数据: %d\n", number);  
    }  
    
    // 终止MPI环境  
    MPI_Finalize();  
    return 0;  
}  
```

#### 编译与执行命令

- **编译**：
    
    ```bash
    mpicc -o point_to_point point_to_point.c  
    ```
    
- **执行**：
    
    ```bash
    mpirun -np 2 ./point_to_point  
    ```
    

或

```bash
mpiexec -n 2 ./point_to_point  
```

### 2. 集体通信 (Collective Communication)

MPI提供了多种集体通信操作，允许多个进程一起工作：
#### 屏障同步 (MPI_Barrier) 
<font color="#7030a0">所有进程必须到达屏障点</font>，直到所有进程都到达后才能继续执行。
```C
int MPI_Barrier(MPI_Comm comm)
```
#### 广播 (Broadcast)

- **功能**：将数据从一个进程（根进程）发送到组内的所有其他进程
- **记忆：** 广播是最简单的传输。只有**一份**要广播的数据，所以只需要**一个缓冲区**、**一个数量**、**一个类型**来描述它。再加上谁是**广播员**(`root`) 和在哪儿广播(`comm`) 就够了。
- **语法**：
    
    ```c
    int MPI_Bcast(      
    void *buffer,         // 数据缓冲区      
    int count,            // 元素数量      
    MPI_Datatype datatype, // 数据类型      
    int root,             // 根进程的rank      
    MPI_Comm comm         // 通信域  
    )  
    ```
    
- **特点**：
    - 所有进程都需要调用此函数
    - 接收进程会接收到与根进程相同的数据

#### 规约 (Reduce)

- **功能**：收集所有进程的数据，进行指定操作（如求和、求最大值等），并将结果发送到指定进程
- `const void* sendbuf`: **每个进程**要贡献的**发送**数据。`const` 表示它不会被修改。
- `void* recvbuf`: 存放**归约结果**的缓冲区。**非常重要：** 这个缓冲区**只在根进程上有效**！非根进程提供 `sendbuf` 后就完成了它的任务，不需要接收最终结果（除非是特殊的 `MPI_IN_PLACE` 情况）。
- `int count`: 每个进程贡献的元素的**数量**。也是结果缓冲区的尺寸（假设是按元素归约）。
- `MPI_Datatype datatype`: 数据的**类型**。
- `MPI_Op op`: **独特的参数！** 指定要做什么**操作**（求和 `MPI_SUM`，求最大值 `MPI_MAX` 等）。
- `int root`: 谁将**接收**最终的归约结果。
- `MPI_Comm comm`: 在哪个组里操作。
- **语法**：
    
    ```c
int MPI_Reduce(
    const void* sendbuf,      // 发送缓冲区 | Send buffer
    void* recvbuf,            // 接收缓冲区 | Receive buffer
    int count,                // 数据项数量 | Number of data items
    MPI_Datatype datatype,    // 数据类型 | Data type
    MPI_Op op,                // 归约操作 | Reduction operation
    int root,                 // 根进程编号 | Root process rank
    MPI_Comm comm             // 通信器 | Communicator
)
 
    ```
    
- **常用规约操作**：
    - `MPI_SUM`: 求和
    - `MPI_MAX`: 求最大值
    - `MPI_MIN`: 求最小值
    - `MPI_PROD`: 求乘积

#### 分散 (Scatter)

- **功能**：将数据从一个进程分发到组内所有进程，每个进程接收不同的数据片段
- `const void* sendbuf`: 根进程上的**完整大块**的**发送**缓冲区。`const` 表示根进程的原始数据不修改。**只有根进程需要提供这个完整的缓冲区。**
    - `int sendcount`: **根进程**发送给**每个**进程的元素的**数量**。（这是**小块**的尺寸）
    - `MPI_Datatype sendtype`: 根进程发送的元素的**类型**。
    - `void* recvbuf`: **每个进程**（包括根进程）用来接收**自己那一份小块**的**接收**缓冲区。
    - `int recvcount`: **每个进程**（包括根进程）期望接收的元素的**数量**。（这通常等于 `sendcount`，但允许类型不同时的数量变化）
    - `MPI_Datatype recvtype`: 每个进程接收的元素的**类型**。
    - `int root`: 谁是**分发者**，谁拥有完整的大块数据。
    - `MPI_Comm comm`: 在哪个组里分散。
- **记忆：** 分散是**从根向外**发。根有**大**的 `sendbuf`，要发给**每个**进程。它发的是**小块**，所以需要 `sendcount` 指定小块尺寸。**每个**进程接收这个小块到自己的 `recvbuf` 里，所以需要 `recvcount`。参数结构是 **`send` 描述符 (3个) + `recv` 描述符 (3个)**，再加上 `root` 和 `comm`。
- **语法**：


```c
int MPI_Scatter(
    const void* sendbuf,      // 发送缓冲区 | Send buffer
    int sendcount,            // 发送给每个进程的数据项数量 | Number of data items to send to each process
    MPI_Datatype sendtype,    // 发送数据类型 | Send data type
    void* recvbuf,            // 接收缓冲区 | Receive buffer
    int recvcount,            // 接收的数据项数量 | Number of data items to receive
    MPI_Datatype recvtype,    // 接收数据类型 | Receive data type
    int root,                 // 根进程编号 | Root process rank
    MPI_Comm comm             // 通信器 | Communicator
)
```


#### 收集 (Gather)

- **功能**：从所有进程收集数据到一个进程，是Scatter的逆操作
- **语法**：
    
    
```c
int MPI_Gather(
    const void* sendbuf,      // 发送缓冲区 | Send buffer
    int sendcount,            // 发送的数据项数量 | Number of data items to send
    MPI_Datatype sendtype,    // 发送数据类型 | Send data type
    void* recvbuf,            // 接收缓冲区 | Receive buffer
    int recvcount,            // 从每个进程接收的数据项数量 | Number of data items to receive from each process
    MPI_Datatype recvtype,    // 接收数据类型 | Receive data type
    int root,                 // 根进程编号 | Root process rank
    MPI_Comm comm             // 通信器 | Communicator
)
```
    

#### 代码示例：集体通信

```c
#include <stdio.h>  
#include <mpi.h>  

int main(int argc, char** argv) {  
    int rank, size;  
    int data;  
    int sum, max_val;  
    int *send_data = NULL;  
    int recv_data;  
    int *gather_data = NULL;  
    
    // 初始化MPI环境  
    MPI_Init(&argc, &argv);  
    MPI_Comm_size(MPI_COMM_WORLD, &size);  
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);  
    
    // 广播示例  
    if (rank == 0) {  
        data = 100;  
        printf("进程0广播数据: %d\n", data);  
    }  
    
    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);  
    printf("进程%d接收到广播数据: %d\n", rank, data);  
    
    // 规约示例 - 求和  
    int local_value = rank + 1;  // 每个进程的本地值  
    
    MPI_Reduce(&local_value, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);  
    
    if (rank == 0) {  
        printf("MPI_SUM规约结果: %d\n", sum);  
    }  
    
    // 规约示例 - 求最大值  
    MPI_Reduce(&local_value, &max_val, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);  
    
    if (rank == 0) {  
        printf("MPI_MAX规约结果: %d\n", max_val);  
    }  
    
    // 分散示例  
    if (rank == 0) {  
        send_data = (int*)malloc(size * sizeof(int));  
        for (int i = 0; i < size; i++) {  
            send_data[i] = i * 10;  
        }  
        printf("进程0准备分散数据\n");  
    }  
    
    MPI_Scatter(send_data, 1, MPI_INT, &recv_data, 1, MPI_INT, 0, MPI_COMM_WORLD);  
    printf("进程%d从分散操作接收到数据: %d\n", rank, recv_data);  
    
    // 收集示例  
    gather_data = NULL;  
    if (rank == 0) {  
        gather_data = (int*)malloc(size * sizeof(int));  
    }  
    
    MPI_Gather(&recv_data, 1, MPI_INT, gather_data, 1, MPI_INT, 0, MPI_COMM_WORLD);  
    
    if (rank == 0) {  
        printf("进程0收集到的数据: ");  
        for (int i = 0; i < size; i++) {  
            printf("%d ", gather_data[i]);  
        }  
        printf("\n");  
        free(gather_data);  
        free(send_data);  
    }  
    
    // 终止MPI环境  
    MPI_Finalize();  
    return 0;  
}  
```

## 六、OpenMP共享内存并行编程

OpenMP是一种用于共享内存并行编程的API，通过指令（pragmas）和库函数支持C/C++和Fortran程序的并行化。

### 1. 主要指令 (Main Directives)

#### parallel指令

- **功能**：创建一组并行执行的线程
- **语法**：
    
    ```c
    #pragma omp parallel [子句列表]  {      // 并行区代码  }  
    ```
    
- **特点**：
    - 创建一个团队的线程
    - 每个线程执行相同的代码块
    - 线程数量可通过环境变量或子句控制

#### for指令

- **功能**：将循环迭代分配给多个线程并行执行
- **语法**：
    
    ```c
    #pragma omp for [子句列表]  for (初始化; 条件; 增量) {      // 循环体  }  
    ```
    
- **特点**：
    - 通常与parallel指令结合使用
    - 可以指定不同的调度策略
    - 循环变量默认为私有变量

#### critical指令

- **功能**：定义一个同一时间只能被一个线程执行的代码区域
- **语法**：
    
    ```c
    #pragma omp critical [名称]  {      // 临界区代码  }  
    ```
    
- **特点**：
    - 用于保护共享资源
    - 防止数据竞争
    - 可以命名不同的临界区

### 2. 主要子句 (Main Clauses)

#### num_threads子句

- **功能**：指定要创建的线程数量
- **语法**：
    
    ```c
    #pragma omp parallel num_threads(线程数)  
    ```
    
- **特点**：
    - 优先级高于环境变量OMP_NUM_THREADS

#### private子句

- **功能**：为每个线程创建变量的私有副本
- **语法**：
    
    ```c
    #pragma omp parallel private(变量列表)  
    ```
    
- **特点**：
    - 每个线程有自己的变量副本
    - 私有变量初始值未定义
    - 退出并行区后，私有变量不再可访问

#### shared子句

- **功能**：指定在所有线程间共享的变量
- **语法**：
    
    ```c
    #pragma omp parallel shared(变量列表)  
    ```
    
- **特点**：
    - 所有线程访问同一变量
    - 需要注意线程安全性
    - 是变量的默认属性（除了循环控制变量）

#### reduction子句

- **功能**：对线程私有副本执行指定操作并合并结果
- **语法**：
    
    ```c
    #pragma omp parallel reduction(操作符:变量列表)  
    ```
    
- **特点**：
    - 支持的操作包括+, *, -, &, |, ^, &&, ||
    - 每个线程有变量的私有副本，初始化为该操作的单位元素
    - 并行区结束后合并所有结果

### 3. 代码示例

#### OpenMP基本并行示例

```c
#include <stdio.h>  
#include <omp.h>  

int main() {  
    // 设置4个线程  
    #pragma omp parallel num_threads(4)  
    {  
        int thread_id = omp_get_thread_num();  
        int total_threads = omp_get_num_threads();  
        
        printf("Hello from thread %d of %d\n", thread_id, total_threads);  
    }  
    
    return 0;  
}  
```

#### 并行for循环示例

```c
#include <stdio.h>  
#include <omp.h>  

int main() {  
    int sum = 0;  
    int i;  
    
    #pragma omp parallel for reduction(+:sum)  
    for (i = 1; i <= 100; i++) {  
        sum += i;  
    }  
    
    printf("1到100的和: %d\n", sum);  
    
    return 0;  
}  
```

#### 使用private和shared子句

```c
#include <stdio.h>  
#include <omp.h>  

int main() {  
    int n = 10;  
    int shared_var = 0;  
    
    #pragma omp parallel num_threads(4) shared(shared_var) private(n)  
    {  
        int thread_id = omp_get_thread_num();  
        
        // n是私有的，每个线程有自己的副本  
        n = thread_id * 10;  
        
        // 更新共享变量需要临界区  
        #pragma omp critical  
        {  
            shared_var += n;  
            printf("线程%d: 私有n=%d, 更新后的shared_var=%d\n",   
                   thread_id, n, shared_var);  
        }  
    }  
    
    printf("最终shared_var值: %d\n", shared_var);  
    printf("最终n值 (主线程): %d\n", n);  // n保持原值10  
    
    return 0;  
}  
```

#### 复杂reduction示例

```c
#include <stdio.h>  
#include <omp.h>  

int main() {  
    int array[100];  
    int sum = 0, min_val = 999999, max_val = -999999;  
    
    // 初始化数组  
    for (int i = 0; i < 100; i++) {  
        array[i] = i * 2;  
    }  
    
    // 多重规约操作  
    #pragma omp parallel for reduction(+:sum) reduction(min:min_val) reduction(max:max_val)  
    for (int i = 0; i < 100; i++) {  
        sum += array[i];  
        
        if (array[i] < min_val) {  
            min_val = array[i];  
        }  
        
        if (array[i] > max_val) {  
            max_val = array[i];  
        }  
    }  
    
    printf("数组元素和: %d\n", sum);  
    printf("最小值: %d\n", min_val);  
    printf("最大值: %d\n", max_val);  
    
    return 0;  
}  
```

### 4. 编译命令

```bash
gcc -fopenmp -o program_name source_file.c  
```

## 总结

1. **通信范围**决定了并行任务如何交换数据，包括点对点通信和集体通信。
    
2. **同步类型**提供了协调并行任务执行的机制，包括屏障同步、互斥锁、信号量和条件变量。
    
3. **粒度**定义了任务分解的规模，影响通信开销和负载均衡，分为细粒度、中粒度和粗粒度。
    
4. **Amdahl定律**揭示了串行部分对最大可能加速比的限制，并帮助解释为什么增加处理器会有收益递减。
    
5. **MPI**提供了在分布式内存系统上进行并行编程的工具，包括点对点通信和集体通信操作。
    
6. **OpenMP**为共享内存系统提供了简单的并行编程方法，通过指令和子句控制线程行为。
    
